{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659600e6",
   "metadata": {},
   "source": [
    "# Patent classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f73403",
   "metadata": {},
   "source": [
    "To begin with, we generate a tiny artificial dataset with 4 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6921610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"This is machine learning related patent\", \"This is a biology related patent\", \"This is machine learning related patent\", \"This is a biology related patent\"]\n",
    "labels = [\"Machine learning\", \"Biology\", \"Machine learning\", \"Biology\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bf5cd0",
   "metadata": {},
   "source": [
    "All the classifiers require the labels to be integers. For this reason, the `LabelsConverter` class can be used to do the needed conversions. The `encode_labels` method converts string labels to int while the `decode_labels` method does the opposite. The latter could be useful to convert the output of the predictions to the more meaningful existing label names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f753f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dapc.labels_converter import LabelsConverter\n",
    "\n",
    "lb = LabelsConverter(labels)\n",
    "lb.classes\n",
    "\n",
    "labels_int = lb.encode_labels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ca3bb7",
   "metadata": {},
   "source": [
    "The simpliest way to train a patent classier is to instantiate a Classifier by specifying the classifier type and its parameters.\n",
    "\n",
    "Below we instantiate one classifier for each of the available categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d7ecd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dic/opt/miniconda3/envs/dapc/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/dic/Software/syngenta-poc/src/dapc/models/transformer_classifier.py:35: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.pre_classifier.weight)\n",
      "/Users/dic/Software/syngenta-poc/src/dapc/models/transformer_classifier.py:36: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.classifier.weight)\n",
      "/Users/dic/opt/miniconda3/envs/dapc/lib/python3.7/site-packages/transformers/adapters/models/bert.py:250: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  FutureWarning,\n",
      "/Users/dic/opt/miniconda3/envs/dapc/lib/python3.7/site-packages/transformers/adapters/models/bert.py:228: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  FutureWarning,\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from dapc.classifier import Classifier\n",
    "\n",
    "transformer_classifier = model = Classifier(\"transformers\", model_name=\"bert-base-uncased\", num_of_labels=len(lb.classes))\n",
    "adapter_classifier = Classifier(\"adapters\", model_name=\"bert-base-uncased\", num_of_labels=len(lb.classes))\n",
    "trans_cnn_classifier = Classifier(\"transformers_cnn\", model_name=\"bert-base-uncased\", num_of_labels=len(lb.classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4887dcf4",
   "metadata": {},
   "source": [
    "All the above classifiers use the default configuration that the respective models hold (Check their implementation for further details). The use of a custom model is also permitted. In this case, someone should initialize the custom model and then pass it directly to the classifier. The example below depicts the instantiation of a custom transformer model with different learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "176dc400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from dapc.models.transformer_classifier import TransformerClassifier\n",
    "transformer_custom_model = TransformerClassifier(classes=3, learning_rate=5e-5)\n",
    "transformer_classifier_custom = Classifier(transformer_custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c65f38",
   "metadata": {},
   "source": [
    "Let's focus on the transformer_classifier and train it using our tiny example dataset. In order to train a classifier we need to provide a list of texts, the respective list of labels and the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f5381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-11 17:18:24.199 | INFO     | dapc.models.transformer_classifier:train:162 - Training Loss per 5000 steps: 0.7248746752738953\n",
      "2022-04-11 17:18:24.200 | INFO     | dapc.models.transformer_classifier:train:163 - Training Accuracy per 5000 steps: 50.0\n",
      "2022-04-11 17:18:26.147 | INFO     | dapc.models.transformer_classifier:train:170 - The Total Accuracy for Epoch 0: 50.0\n",
      "2022-04-11 17:18:26.148 | INFO     | dapc.models.transformer_classifier:train:174 - Training Loss Epoch: 0.7248746752738953\n",
      "2022-04-11 17:18:26.149 | INFO     | dapc.models.transformer_classifier:train:175 - Training Accuracy Epoch: 50.0\n"
     ]
    }
   ],
   "source": [
    "transformer_classifier.train(texts,labels_int, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cee5ec",
   "metadata": {},
   "source": [
    "The accuracy is zero as the dataset is dummy but let's pretend that the model has been trained succesfully and we would like to evaluate it. This can be done by using the `evaluate` method and passing to it a set of texts and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "836bf4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 1.0,\n",
       " 'macro_recall': 1.0,\n",
       " 'macro_f1_score': 1.0,\n",
       " 'micro_precision': 1.0,\n",
       " 'micro_recall': 1.0,\n",
       " 'micro_f1_score': 1.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_classifier.evaluate(texts,labels_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5280b2b",
   "metadata": {},
   "source": [
    "Respectively for prediction, we can use the `predict` method and providing the list of texts of interest. The output of the prediction is the predicted labels together with the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae98042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, logits = transformer_classifier.predict(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4127401",
   "metadata": {},
   "source": [
    "Let's use the decode_labels methood to convert the labels to their names and inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "894137f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine learning', 'Biology', 'Machine learning', 'Biology']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.decode_labels(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f29852",
   "metadata": {},
   "source": [
    "To cross validate the classifier, someone can use the `cross_validation` method. In addition to the texts,labels and epochs, number of cross validations(k) should also be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0fb6f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/dic/Software/syngenta-poc/src/dapc/models/transformer_classifier.py:35: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.pre_classifier.weight)\n",
      "/Users/dic/Software/syngenta-poc/src/dapc/models/transformer_classifier.py:36: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.classifier.weight)\n",
      "2022-04-11 17:18:32.958 | INFO     | dapc.models.transformer_classifier:train:162 - Training Loss per 5000 steps: 0.919713020324707\n",
      "2022-04-11 17:18:32.959 | INFO     | dapc.models.transformer_classifier:train:163 - Training Accuracy per 5000 steps: 50.0\n",
      "2022-04-11 17:18:34.055 | INFO     | dapc.models.transformer_classifier:train:170 - The Total Accuracy for Epoch 0: 50.0\n",
      "2022-04-11 17:18:34.056 | INFO     | dapc.models.transformer_classifier:train:174 - Training Loss Epoch: 0.919713020324707\n",
      "2022-04-11 17:18:34.057 | INFO     | dapc.models.transformer_classifier:train:175 - Training Accuracy Epoch: 50.0\n",
      "2022-04-11 17:18:34.170 | INFO     | dapc.models.transformer_classifier:train:162 - Training Loss per 5000 steps: 0.39780521392822266\n",
      "2022-04-11 17:18:34.171 | INFO     | dapc.models.transformer_classifier:train:163 - Training Accuracy per 5000 steps: 100.0\n",
      "2022-04-11 17:18:34.999 | INFO     | dapc.models.transformer_classifier:train:170 - The Total Accuracy for Epoch 1: 100.0\n",
      "2022-04-11 17:18:35.001 | INFO     | dapc.models.transformer_classifier:train:174 - Training Loss Epoch: 0.39780521392822266\n",
      "2022-04-11 17:18:35.001 | INFO     | dapc.models.transformer_classifier:train:175 - Training Accuracy Epoch: 100.0\n",
      "2022-04-11 17:18:35.115 | INFO     | dapc.models.transformer_classifier:train:162 - Training Loss per 5000 steps: 0.6558307409286499\n",
      "2022-04-11 17:18:35.116 | INFO     | dapc.models.transformer_classifier:train:163 - Training Accuracy per 5000 steps: 50.0\n",
      "2022-04-11 17:18:36.108 | INFO     | dapc.models.transformer_classifier:train:170 - The Total Accuracy for Epoch 2: 50.0\n",
      "2022-04-11 17:18:36.109 | INFO     | dapc.models.transformer_classifier:train:174 - Training Loss Epoch: 0.6558307409286499\n",
      "2022-04-11 17:18:36.110 | INFO     | dapc.models.transformer_classifier:train:175 - Training Accuracy Epoch: 50.0\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/dic/Software/syngenta-poc/src/dapc/models/transformer_classifier.py:35: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.pre_classifier.weight)\n",
      "/Users/dic/Software/syngenta-poc/src/dapc/models/transformer_classifier.py:36: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.classifier.weight)\n",
      "2022-04-11 17:18:43.035 | INFO     | dapc.models.transformer_classifier:train:162 - Training Loss per 5000 steps: 0.5839182734489441\n",
      "2022-04-11 17:18:43.035 | INFO     | dapc.models.transformer_classifier:train:163 - Training Accuracy per 5000 steps: 50.0\n",
      "2022-04-11 17:18:44.017 | INFO     | dapc.models.transformer_classifier:train:170 - The Total Accuracy for Epoch 0: 50.0\n",
      "2022-04-11 17:18:44.018 | INFO     | dapc.models.transformer_classifier:train:174 - Training Loss Epoch: 0.5839182734489441\n",
      "2022-04-11 17:18:44.018 | INFO     | dapc.models.transformer_classifier:train:175 - Training Accuracy Epoch: 50.0\n",
      "2022-04-11 17:18:44.124 | INFO     | dapc.models.transformer_classifier:train:162 - Training Loss per 5000 steps: 0.3370954394340515\n",
      "2022-04-11 17:18:44.125 | INFO     | dapc.models.transformer_classifier:train:163 - Training Accuracy per 5000 steps: 100.0\n",
      "2022-04-11 17:18:44.938 | INFO     | dapc.models.transformer_classifier:train:170 - The Total Accuracy for Epoch 1: 100.0\n",
      "2022-04-11 17:18:44.939 | INFO     | dapc.models.transformer_classifier:train:174 - Training Loss Epoch: 0.3370954394340515\n",
      "2022-04-11 17:18:44.941 | INFO     | dapc.models.transformer_classifier:train:175 - Training Accuracy Epoch: 100.0\n",
      "2022-04-11 17:18:45.044 | INFO     | dapc.models.transformer_classifier:train:162 - Training Loss per 5000 steps: 0.6490230560302734\n",
      "2022-04-11 17:18:45.044 | INFO     | dapc.models.transformer_classifier:train:163 - Training Accuracy per 5000 steps: 50.0\n",
      "2022-04-11 17:18:45.846 | INFO     | dapc.models.transformer_classifier:train:170 - The Total Accuracy for Epoch 2: 50.0\n",
      "2022-04-11 17:18:45.847 | INFO     | dapc.models.transformer_classifier:train:174 - Training Loss Epoch: 0.6490230560302734\n",
      "2022-04-11 17:18:45.848 | INFO     | dapc.models.transformer_classifier:train:175 - Training Accuracy Epoch: 50.0\n",
      "2022-04-11 17:18:45.944 | INFO     | dapc.classifier:cross_validation:211 - macro_precision:   min: 1.0000  max: 1.0000  mean: 1.0000  std: 0.0000\n",
      "2022-04-11 17:18:45.944 | INFO     | dapc.classifier:cross_validation:211 - macro_recall:   min: 1.0000  max: 1.0000  mean: 1.0000  std: 0.0000\n",
      "2022-04-11 17:18:45.945 | INFO     | dapc.classifier:cross_validation:211 - macro_f1_score:   min: 1.0000  max: 1.0000  mean: 1.0000  std: 0.0000\n",
      "2022-04-11 17:18:45.946 | INFO     | dapc.classifier:cross_validation:211 - micro_precision:   min: 1.0000  max: 1.0000  mean: 1.0000  std: 0.0000\n",
      "2022-04-11 17:18:45.946 | INFO     | dapc.classifier:cross_validation:211 - micro_recall:   min: 1.0000  max: 1.0000  mean: 1.0000  std: 0.0000\n",
      "2022-04-11 17:18:45.947 | INFO     | dapc.classifier:cross_validation:211 - micro_f1_score:   min: 1.0000  max: 1.0000  mean: 1.0000  std: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'macro_precision': [1.0, 1.0],\n",
       " 'macro_recall': [1.0, 1.0],\n",
       " 'macro_f1_score': [1.0, 1.0],\n",
       " 'micro_precision': [1.0, 1.0],\n",
       " 'micro_recall': [1.0, 1.0],\n",
       " 'micro_f1_score': [1.0, 1.0]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_classifier.cross_validation(texts,labels_int,k=2,epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456319b2",
   "metadata": {},
   "source": [
    "Once the training is done, we can save the model using the `save` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f15b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_classifier.save(\"my_beatiful_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2539347",
   "metadata": {},
   "source": [
    "Then you can reload it using the `load` method and use it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e79629b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 1.0,\n",
       " 'macro_recall': 1.0,\n",
       " 'macro_f1_score': 1.0,\n",
       " 'micro_precision': 1.0,\n",
       " 'micro_recall': 1.0,\n",
       " 'micro_f1_score': 1.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_classifier.load(\"my_beatiful_classifier\")\n",
    "transformer_classifier.evaluate(texts,labels_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f8b9d3",
   "metadata": {},
   "source": [
    "Note that as during cross validation more than one models are trained, we suggest to not use the save method after the cross validation. Instead you can define a save model strategy during the cross validation by using the `save_model_path` and `save_model_strategy` parameters of the cross_validation method. Specifically `save_model_path` defines the path where the checkpoins will be saved while the `save_model_strategy` defines the strategy based on which they are going to be saved. For instance, if someone wishes to store all the models then should define `save_model_strategy='all'`. Alternative, if the goal is to save the best model based on a specific metric then the name of the metric should be provided, for example:  `save_model_strategy='micro_f1_score'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b56f6",
   "metadata": {},
   "source": [
    "### Multilingual patent classification\n",
    "\n",
    "The package supports multilingual training. All the information that have been presented in the monolingual notebook stand also for this case. The only addition that the multilingual classifier holds is the ability to perform per language evaluation of the model's performance.\n",
    "\n",
    "Firstly, let's create an instance of the multilingual classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60dc0db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/dic/Software/syngenta-poc/src/dapc/models/transformer_classifier.py:35: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.pre_classifier.weight)\n",
      "/Users/dic/Software/syngenta-poc/src/dapc/models/transformer_classifier.py:36: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.classifier.weight)\n"
     ]
    }
   ],
   "source": [
    "from dapc.classifier_multilingual import ClassifierMultilingual\n",
    "\n",
    "transformer_classifier_multi = ClassifierMultilingual(\"transformers\", model_name=\"bert-base-uncased\", num_of_labels=len(lb.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc5e861",
   "metadata": {},
   "source": [
    "Then, let's stick on the already existing small dataset and consider that it is a multilingual dataset. For a multilingual evaluation, we need a list of languages of the input texts. If we do not have this inforation available already, the package contains the LanguageDetector class to do this for you relying on spacy. \n",
    "\n",
    "<u>Note: before running the language detector for the first time execute the following command to download the needed model:\n",
    "`python -m spacy download en_core_web_sm`!</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4316b121",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x7/q0yyr8y149s44p_hlr2g__6r0000kp/T/ipykernel_45320/1872740454.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdapc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguages_detector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLanguagesDetector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlan_detector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguagesDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlangs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlan_detector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_languages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlangs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/syngenta-poc/src/dapc/languages_detector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;34m\"\"\"Create a languages detector class.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mLanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"language_detector\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_lang_detector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"language_detector\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/dapc/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[1;32m     51\u001b[0m     return util.load_model(\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/dapc/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "from dapc.languages_detector import LanguagesDetector\n",
    "\n",
    "lan_detector = LanguagesDetector()\n",
    "langs = lan_detector.infer_languages(texts)\n",
    "langs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8257ace",
   "metadata": {},
   "source": [
    "Once the training set and their languages are known, we can train the model or use the multilingual cross validation and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1386ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_classifier_multi.cross_validation_multilingual(texts,labels_int, langs,k=2,epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
